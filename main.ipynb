{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST data classification using denoising autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "m5mLZUzGgjOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Denoising Autoencoders for MNIST classification"
      ]
    },
    {
      "metadata": {
        "id": "3BxNJnDLgqns",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tvrm1ZmwgwsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "BatchSize = 1000\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
        "                                          shuffle=True, num_workers=4) # Creating dataloader\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./MNIST', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
        "                                         shuffle=False, num_workers=4) # Creating dataloader\n",
        "\n",
        "classes = ('zero', 'one', 'two', 'three',\n",
        "           'four', 'five', 'six', 'seven', 'eight', 'nine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9p7K_MMdgwiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d04b422c-932d-4ff0-9dc4-6c536a57ceec"
      },
      "cell_type": "code",
      "source": [
        "# Check availability of GPU\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print('GPU is available!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F0mE4mrfg29A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the Autoencoder:"
      ]
    },
    {
      "metadata": {
        "id": "IOA0cRVpg1my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L1Penalty(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(self, input, l1weight):\n",
        "        self.save_for_backward(input)\n",
        "        self.l1weight = l1weight\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        input, = self.saved_variables\n",
        "        grad_input = input.clone().sign().mul(self.l1weight)\n",
        "        grad_input += grad_output\n",
        "        return grad_input, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPau33QYjKQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "1005ef89-5dff-4c0d-ea00-e162a77aca3e"
      },
      "cell_type": "code",
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28*28, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 128),\n",
        "            nn.ReLU())\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(128,400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400,28*28),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "       # x = L1Penalty.apply(x, 0.1) # Uncomment it to add 10% sparsity in the autoencoder\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = autoencoder()\n",
        "print(net)\n",
        "\n",
        "if use_gpu:\n",
        "    net = net.double().cuda()\n",
        "else:\n",
        "    net = net.double()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=400, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=400, out_features=784, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wlR163W1hGz4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Autoencoder"
      ]
    },
    {
      "metadata": {
        "id": "7a_ZzBeShCL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "outputId": "b943094c-0e72-4631-88f3-daefbcf9f85d"
      },
      "cell_type": "code",
      "source": [
        "iterations = 50\n",
        "learning_rate = 1e-3\n",
        "noise_mean = 0.1\n",
        "noise_std = 0.2\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = learning_rate) # Adam optimizer for optimization\n",
        "for epoch in range(iterations):  # loop over the dataset multiple times\n",
        "\n",
        "    runningLoss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        \n",
        "        # wrap them in Variable\n",
        "        if use_gpu:\n",
        "            ideal_outputs = Variable(inputs.view(-1, 28*28).double()).cuda()\n",
        "            # Noise\n",
        "            noise = Variable(ideal_outputs.data.new(ideal_outputs.size()).normal_(noise_mean, noise_std).double()).cuda()\n",
        "            # Adding Noise (Noisy Input)\n",
        "            inputs = Variable(torch.clamp((ideal_outputs + noise).data,0,1).double()).cuda()\n",
        "        else:\n",
        "            ideal_outputs = Variable(inputs.view(-1, 28*28).double())\n",
        "            noise = Variable(ideal_outputs.data.new(ideal_outputs.size()).normal_(noise_mean, noise_std).double())\n",
        "            inputs = Variable(torch.clamp((ideal_outputs + noise).data,0,1).double())\n",
        "\n",
        "        optimizer.zero_grad()  # zeroes the gradient buffers of all parameters\n",
        "        outputs = net(inputs) # forward \n",
        "        loss = criterion(outputs, ideal_outputs) # calculate loss\n",
        "        loss.backward() #  backpropagate the loss\n",
        "        optimizer.step()\n",
        "        runningLoss += loss.data\n",
        "    print('At Iteration : %d / %d  ;  Mean-Squared Error : %f'%(epoch + 1,iterations,\n",
        "                                                                        runningLoss/(60000/BatchSize)))\n",
        "print('Finished Training')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At Iteration : 1 / 50  ;  Mean-Squared Error : 0.088127\n",
            "At Iteration : 2 / 50  ;  Mean-Squared Error : 0.059990\n",
            "At Iteration : 3 / 50  ;  Mean-Squared Error : 0.044471\n",
            "At Iteration : 4 / 50  ;  Mean-Squared Error : 0.035104\n",
            "At Iteration : 5 / 50  ;  Mean-Squared Error : 0.029682\n",
            "At Iteration : 6 / 50  ;  Mean-Squared Error : 0.025729\n",
            "At Iteration : 7 / 50  ;  Mean-Squared Error : 0.023150\n",
            "At Iteration : 8 / 50  ;  Mean-Squared Error : 0.021022\n",
            "At Iteration : 9 / 50  ;  Mean-Squared Error : 0.019584\n",
            "At Iteration : 10 / 50  ;  Mean-Squared Error : 0.018237\n",
            "At Iteration : 11 / 50  ;  Mean-Squared Error : 0.017084\n",
            "At Iteration : 12 / 50  ;  Mean-Squared Error : 0.016200\n",
            "At Iteration : 13 / 50  ;  Mean-Squared Error : 0.015440\n",
            "At Iteration : 14 / 50  ;  Mean-Squared Error : 0.014751\n",
            "At Iteration : 15 / 50  ;  Mean-Squared Error : 0.014080\n",
            "At Iteration : 16 / 50  ;  Mean-Squared Error : 0.013492\n",
            "At Iteration : 17 / 50  ;  Mean-Squared Error : 0.013046\n",
            "At Iteration : 18 / 50  ;  Mean-Squared Error : 0.012619\n",
            "At Iteration : 19 / 50  ;  Mean-Squared Error : 0.012259\n",
            "At Iteration : 20 / 50  ;  Mean-Squared Error : 0.011909\n",
            "At Iteration : 21 / 50  ;  Mean-Squared Error : 0.011615\n",
            "At Iteration : 22 / 50  ;  Mean-Squared Error : 0.011317\n",
            "At Iteration : 23 / 50  ;  Mean-Squared Error : 0.011048\n",
            "At Iteration : 24 / 50  ;  Mean-Squared Error : 0.010846\n",
            "At Iteration : 25 / 50  ;  Mean-Squared Error : 0.010570\n",
            "At Iteration : 26 / 50  ;  Mean-Squared Error : 0.010377\n",
            "At Iteration : 27 / 50  ;  Mean-Squared Error : 0.010133\n",
            "At Iteration : 28 / 50  ;  Mean-Squared Error : 0.009921\n",
            "At Iteration : 29 / 50  ;  Mean-Squared Error : 0.009753\n",
            "At Iteration : 30 / 50  ;  Mean-Squared Error : 0.009547\n",
            "At Iteration : 31 / 50  ;  Mean-Squared Error : 0.009389\n",
            "At Iteration : 32 / 50  ;  Mean-Squared Error : 0.009270\n",
            "At Iteration : 33 / 50  ;  Mean-Squared Error : 0.009048\n",
            "At Iteration : 34 / 50  ;  Mean-Squared Error : 0.008922\n",
            "At Iteration : 35 / 50  ;  Mean-Squared Error : 0.008782\n",
            "At Iteration : 36 / 50  ;  Mean-Squared Error : 0.008733\n",
            "At Iteration : 37 / 50  ;  Mean-Squared Error : 0.008560\n",
            "At Iteration : 38 / 50  ;  Mean-Squared Error : 0.008413\n",
            "At Iteration : 39 / 50  ;  Mean-Squared Error : 0.008328\n",
            "At Iteration : 40 / 50  ;  Mean-Squared Error : 0.008238\n",
            "At Iteration : 41 / 50  ;  Mean-Squared Error : 0.008146\n",
            "At Iteration : 42 / 50  ;  Mean-Squared Error : 0.008032\n",
            "At Iteration : 43 / 50  ;  Mean-Squared Error : 0.007977\n",
            "At Iteration : 44 / 50  ;  Mean-Squared Error : 0.007860\n",
            "At Iteration : 45 / 50  ;  Mean-Squared Error : 0.007832\n",
            "At Iteration : 46 / 50  ;  Mean-Squared Error : 0.007764\n",
            "At Iteration : 47 / 50  ;  Mean-Squared Error : 0.007644\n",
            "At Iteration : 48 / 50  ;  Mean-Squared Error : 0.007618\n",
            "At Iteration : 49 / 50  ;  Mean-Squared Error : 0.007538\n",
            "At Iteration : 50 / 50  ;  Mean-Squared Error : 0.007450\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWQMxySHhRte",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Autoencoder Performance:"
      ]
    },
    {
      "metadata": {
        "id": "5LX7pnu1hTqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "8e681248-9c3b-4f90-f92d-bd7275e077d7"
      },
      "cell_type": "code",
      "source": [
        "# functions to show an image\n",
        "def imshow(img, strlabel):\n",
        "    npimg = img.numpy()\n",
        "    npimg = np.abs(npimg)\n",
        "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "    fig_size[0] = 10\n",
        "    fig_size[1] = 10\n",
        "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "    plt.figure()\n",
        "    plt.title(strlabel)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "if use_gpu:\n",
        "    ideal_outputs = Variable(images[0].view(-1,28*28).double()).cuda()\n",
        "    noise = Variable(ideal_outputs.data.new(ideal_outputs.size()).normal_(noise_mean, noise_std).double()).cuda()\n",
        "    inputs = ideal_outputs + noise\n",
        "    outImg = net(inputs).data\n",
        "    outImg = outImg.view(-1,28,28).cpu()\n",
        "else:\n",
        "    ideal_outputs = Variable(images[0].view(-1,28*28).double())\n",
        "    noise = Variable(ideal_outputs.data.new(ideal_outputs.size()).normal_(noise_mean, noise_std).double())\n",
        "    inputs = ideal_outputs + noise\n",
        "    outImg = net(inputs).data\n",
        "    outImg = outImg.view(-1,28,28)\n",
        "\n",
        "dispImg = torch.Tensor(2,1,28,28)\n",
        "dispImg[0] = torch.clamp(inputs.data.view(-1,28,28).cpu(),0,1)\n",
        "dispImg[1] = outImg\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(dispImg), 'Noisy Input                                              Denoised Output')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAFOCAYAAABwsZJoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACcKJwAAACc/j8r5Vhsn5sHigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "rSp3EgFEhoTy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modifying the autoencoder for classification:"
      ]
    },
    {
      "metadata": {
        "id": "91ZWOhruhp3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "8c71d880-b1cc-4136-d0c8-2dddb9a5d44c"
      },
      "cell_type": "code",
      "source": [
        "# Removing the decoder module from the autoencoder\n",
        "new_classifier = nn.Sequential(*list(net.children())[:-1])\n",
        "net = new_classifier\n",
        "# Adding linear layer for 10-class classification problem\n",
        "net.add_module('classifier', nn.Sequential(nn.Linear(128, 64),nn.ReLU(),\n",
        "                                          nn.Linear(64, 10),nn.LogSoftmax()))\n",
        "print(net)\n",
        "if use_gpu:\n",
        "    net = net.double().cuda()\n",
        "else:\n",
        "    net = net.double()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=400, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (3): LogSoftmax()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mIr_qZLlhwHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Classifier"
      ]
    },
    {
      "metadata": {
        "id": "_b7YVOPXhsI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4ce0b80c-2489-44f3-a037-59aa3159b927"
      },
      "cell_type": "code",
      "source": [
        "iterations = 50\n",
        "learning_rate = 0.1\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "for epoch in range(iterations):  # loop over the dataset multiple times\n",
        "\n",
        "    runningLoss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        \n",
        "        # wrap them in Variable\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.view(-1, 28*28).double()).cuda(), Variable(labels).cuda()\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs.view(-1, 28*28).double()), Variable(labels)\n",
        "\n",
        "        net.zero_grad()  # zeroes the gradient buffers of all parameters\n",
        "        outputs = net(inputs) # forward \n",
        "        loss = criterion(outputs, labels) # calculate loss\n",
        "        loss.backward() #  backpropagate the loss\n",
        "        for f in net.parameters():\n",
        "            f.data.sub_(f.grad.data * learning_rate) # weight = weight - learning_rate * gradient (Update Weights)\n",
        "        runningLoss += loss.data\n",
        "        correct = 0\n",
        "        total = 0\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.view(-1, 28*28).double()).cuda(), labels.cuda()\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs.view(-1, 28*28).double()), labels\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "    print('At Iteration : %d / %d  ;  Train Error : %f ;Test Accuracy : %f'%(epoch + 1,iterations,\n",
        "                                                                        runningLoss/(60000/BatchSize),100 *(float(correct) /float(total))))\n",
        "print('Finished Training')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "At Iteration : 1 / 3  ;  Train Error : 0.059053 ;Test Accuracy : 97.400000\n",
            "At Iteration : 2 / 3  ;  Train Error : 0.058105 ;Test Accuracy : 97.390000\n",
            "At Iteration : 3 / 3  ;  Train Error : 0.057216 ;Test Accuracy : 97.460000\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}